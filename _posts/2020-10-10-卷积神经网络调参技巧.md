# 卷积神经网络调参技巧

​		卷积神经网络的架构及超参数决定了模型的能力,以及针对特定输入效果的好坏。本文所说的调参不仅包括超参数的调节,还包括神经网络架构的搭建。

​		首先针对**模型架构的搭建**，主要是说神经网络模型各层之间的关系，或者说是输入输出的结构（这一点有待补充），目前针对不同的任务（图片分类、目标检测、语义分割）已经发展出了比较成熟的模型，这些模型往往是经过精心调节的，并且在实践中取得了比较好的效果，可以参考[计算机视觉和 CNN 发展十一座里程碑](https://mp.weixin.qq.com/s/eosTWBbLpwVroYPEb9Q0wA?)。几乎每个在前人之上有所突破的模型都会在架构上引入新的思想或技巧（有待举例论证），了解这些改变的意义会在调节模型架构上非常有帮助。以我的观点上，卷积神经网络层的选取、层级之间的关系、损失函数、优化器、正则项等等都能算是神经网络架构的一部分。在神经网络层上，除了最为熟悉的卷积层和全连接层之外，还有池化层、激活层、softmax层、规范层、噪声层等等。此外很多创新往往是在现有层的基础上进行改进的，比如为了适应输入形式引入了一维和三维卷积层，还有空洞卷积层等等，具体改进所带来的效果可参考各文献。

​		另外就是**超参数的调节**，卷积神经网络中包含许多超参数，它包括各层的超参数（比如卷积核大小、卷积核个数，步长，padding大小、池化层范围等）以及训练超参数（比如学习率，训练次数等等）， 在选定了神经网络的架构之后，剩下的就是调节超参数。超参数的调节也是有规律可循的，

​		理论上说整个模型层数越多，体型越庞大，那么它的拟合能力越强，但这仅仅是理论上，当整个模型层数深到一定程度，往往实验效果会变差（原因有待探究）。

------

参考文章：

[卷积神经网络（AlexNet）调参技巧总结](https://blog.csdn.net/u011268787/article/details/85043511)以AlexNet神经网络为例,讲了一些比较主观，加上一些实验辅助的论断

> - 卷积核
>
> 卷积核是卷积神经网络的核心，卷积核的大小设置，我都是设置为3×3，一个方面是因为我的图片比较小，还有一个原因是这个卷积核是最小的并且能够体现出上下左右中方位信息的卷积核。
>
> - 池化层
>
> 池化层的窗口大小是2×2，可以搭配stride =2 的步幅，这是为了层加深就需要使下采样速度更慢。
>
> - 层数
>
> 开始的代码是三层卷积层，但是效果一直都一般，后来没办法，我添加了一个卷积层。卷积层达到了四层，而卷积核有五个。一般来说，卷积核的个数最好是奇数个
>
> **全连接层**
>
> dropout层放置的位置一般都要放置在全连接层中。全连接层的`dropout`层的参数设置,我的个人经验是，不要设置地太大，我一般设置在`0.1~0.3`之间。

[卷积神经网络的卷积核大小、个数，卷积层数如何确定呢？](https://www.sohu.com/a/241208957_787107)

> 在达到相同感受野的情况下，卷积核越小，所需要的参数和计算量越小。具体来说。卷积核大小必须大于1才有提升感受野的作用，1排除了。而大小为偶数的卷积核即使对称地加padding也不能保证输入feature map尺寸和输出feature map尺寸不变（画个图算一下就可以发现），2排除了。所以一般都用3作为卷积核大小。
>
> 每一层卷积有多少channel数，以及一共有多少层卷积，这些暂时没有理论支撑，一般都是靠感觉去设置几组候选值，然后通过实验挑选出其中的最佳值。这也是现在深度卷积神经网络虽然效果拔群，但是一直为人诟病的原因之一。
>
> 调参就是trial-and-error. 没有其他捷径可以走. 唯一的区别是有些人盲目的尝试, 有些人思考后再尝试. 快速尝试, 快速纠错这是调参的关键.
>
> * 可视化
>
> 我个人的理解, 对于可视化, 更多的还是帮助人类以自己熟悉的方式来观察网络. 因为, 你是不可能边观察网络, 还边调参的. 你只是训练完成后(或者准确率到达一个阶段后), 才能可视化. 
>
> - 可视化网络就不重要了?
>
> 非常重要, 但是不在训练这块, 而是帮助理解网络的原理这块. 理解网络原理后, 你才能在设计结构的时候心里有感觉(只是有感觉而已), 网络出了问题, 或者在某些情况下不满意, 有更好的直觉去调整.
>
> #### 一些大的注意事项:
>
> 1. 刚开始, 先上小规模数据, 模型往大了放, 只要不爆显存, 能用256个filter你就别用128个. 直接奔着过拟合去. 没错, 就是训练过拟合网络, 连测试集验证集这些都可以不用.
>
> * 你要验证自己的训练脚本的流程对不对. 这一步小数据量, 生成速度快, 但是所有的脚本都是和未来大规模训练一致的(除了少跑点循环)
>
> * 如果小数据量下, 你这么粗暴的大网络奔着过拟合去都没效果. 那么, 你要开始反思自己了, 模型的输入输出是不是有问题? 要不要检查自己的代码(永远不要怀疑工具库, 除非你动过代码)? 模型解决的问题定义是不是有问题? 你对应用场景的理解是不是有错? 不要怀疑NN的能力, 不要怀疑NN的能力, 不要怀疑NN的能力. 就我们调参狗能遇到的问题, NN没法拟合的, 这概率是有多小?
>
> * 你可以不这么做, 但是等你数据准备了两天, 结果发现有问题要重新生成的时候, 你这周时间就酱油了.
>
> 2. Loss设计要合理.
>
> * 一般来说分类就是Softmax, 回归就是L2的loss. 但是要注意loss的错误范围(主要是回归), 你预测一个label是10000的值, 模型输出0, 你算算这loss多大, 这还是单变量的情况下. 一般结果都是nan. 所以不仅仅输入要做normalization, 输出也要这么弄.
>
> * 多任务情况下, 各loss想法限制在一个量级上, 或者最终限制在一个量级上, 初期可以着重一个任务的loss
>
> 3. 观察loss胜于观察准确率
>
> * 准确率虽然是评测指标, 但是训练过程中还是要注意loss的. 你会发现有些情况下, 准确率是突变的, 原来一直是0, 可能保持上千迭代, 然后突然变1. 要是因为这个你提前中断训练了, 只有老天替你惋惜了. 而loss是不会有这么诡异的情况发生的, 毕竟优化目标是loss.
>
> * 给NN一点时间, 要根据任务留给NN的学习一定空间. 不能说前面一段时间没起色就不管了. 有些情况下就是前面一段时间看不出起色, 然后开始稳定学习.
>
> 4. 确认分类网络学习充分
>
> * 分类网络就是学习类别之间的界限. 你会发现, 网络就是慢慢的从类别模糊到类别清晰的. 怎么发现? 看Softmax输出的概率的分布. 如果是二分类, 你会发现, 刚开始的网络预测都是在0.5上下, 很模糊. 随着学习过程, 网络预测会慢慢的移动到0,1这种极值附近. 所以, 如果你的网络预测分布靠中间, 再学习学习.
>
> 5. Learning Rate设置合理
>
> * 太大: loss爆炸, 或者nan
>
> * 太小: 半天loss没反映(但是, LR需要降低的情况也是这样, 这里可视化网络中间结果, 不是weights, 有效果, 俩者可视化结果是不一样的, 太小的话中间结果有点水波纹或者噪点的样子, 因为filter学习太慢的原因, 试过就会知道很明显)
>
> * 需要进一步降低了: loss在当前LR下一路降了下来, 但是半天不再降了.
>
> * 如果有个复杂点的任务, 刚开始, 是需要人肉盯着调LR的. 后面熟悉这个任务网络学习的特性后, 可以扔一边跑去了.
>
> * 如果上面的Loss设计那块你没法合理, 初始情况下容易爆, 先上一个小LR保证不爆, 等loss降下来了, 再慢慢升LR, 之后当然还会慢慢再降LR, 虽然这很蛋疼.
>
> * LR在可以工作的最大值下往小收一收, 免得ReLU把神经元弄死了. 当然, 我是个心急的人, 总爱设个大点的.
>
> 6. 对比训练集和验证集的loss
>
> * 判断过拟合, 训练是否足够, 是否需要early stop的依据, 这都是中规中矩的原则, 不多说了.
>
> 7. 清楚receptive field的大小
>
> * CV的任务, context window是很重要的. 所以你对自己模型的receptive field的大小要心中有数. 这个对效果的影响还是很显著的. 特别是用FCN, 大目标需要很大的receptive field. 不像有fully connection的网络, 好歹有个fc兜底, 全局信息都有.
>
> #### 简短的注意事项: 
>
> 1. 预处理: -mean/std zero-center就够了, PCA, 白化什么的都用不上. 我个人观点, 反正CNN能学习encoder, PCA用不用其实关系不大, 大不了网络里面自己学习出来一个.
> 2. shuffle, shuffle, shuffle.
> 3. 网络原理的理解最重要, CNN的conv这块, 你得明白sobel算子的边界检测.
> 4. Dropout, Dropout, Dropout(不仅仅可以防止过拟合, 其实这相当于做人力成本最低的Ensemble, 当然, 训练起来会比没有Dropout的要慢一点, 同时网络参数你最好相应加一点, 对, 这会再慢一点).
> 5. CNN更加适合训练回答是否的问题, 如果任务比较复杂, 考虑先用分类任务训练一个模型再finetune.
> 6. 无脑用ReLU(CV领域).
> 7. 无脑用3x3.
> 8. 无脑用xavier.
> 9. LRN一类的, 其实可以不用. 不行可以再拿来试试看.
> 10. filter数量2^n.
> 11. 多尺度的图片输入(或者网络内部利用多尺度下的结果)有很好的提升效果.
> 12. 第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要).
> 13. sgd adam 这些选择上, 看你个人选择. 一般对网络不是决定性的. 反正我无脑用sgd + momentum.
> 14. batch normalization我一直没用, 虽然我知道这个很好, 我不用仅仅是因为我懒. 所以要鼓励使用batch normalization.
> 15. 不要完全相信论文里面的东西. 结构什么的觉得可能有效果, 可以拿去试试.
> 16. 你有95%概率不会使用超过40层的模型.
> 17. shortcut的联接是有作用的.
> 18. 暴力调参最可取, 毕竟, 自己的生命最重要. 你调完这个模型说不定过两天这模型就扔掉了.
> 19. 机器, 机器, 机器.
> 20. Google的inception论文, 结构要好好看看.
> 21. 一些传统的方法, 要稍微了解了解. 我自己的程序就用过1x14的手写filter, 写过之后你看看inception里面的1x7, 7x1 就会会心一笑...

[卷积神经网络超详细介绍]([https://blog.csdn.net/jiaoyangwm/article/details/80011656?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduend~default-6-80011656.nonecase&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82&spm=1000.2123.3001.4430](https://blog.csdn.net/jiaoyangwm/article/details/80011656?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduend~default-6-80011656.nonecase&utm_term=卷积神经网络调参&spm=1000.2123.3001.4430))这篇文章梳理了卷积神经网络的发展,值得参考









