## 特征工程

[TOC]

### 特征选择

我们将事物的属性称为**特征**，对当前学习任务有用的属性称为**相关特征**，没有什么用的属性称为**无关特征**，从给定的特征集合中选出相关特征子集的过程称为**特征选择（feature selection）**。还有一种特征是**冗余特征**，它所包含的信息能从其他的特征中推演出来。

特征选择是一个重要的**数据预处理**过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器。

进行特征选择的**原因**如下：

* 在现实中容易遇到维数灾难问题，这是由于属性过多造成的。若能从中选择比较重要的特征进行学习，只需要在一部分特征上构建模型，则维数灾难问题会大为减轻。
* 去除不相关特征往往会降低学习任务的难度。

### 特征选择的方式

特征选择的可行做法是产生一个**候选子集**，评价出它的好坏，基于评价结果产生下一个候选子集，再对其进行评价，循环此过程直至无法找到更好的子集为止。这里有两个关键环节：

* 子集搜索（subset search）：逐渐增加相关特征称为**前向搜索**，逐渐减少相关特征称为**后向搜索**，每一轮逐渐增加选定相关特征，同时减少无关特征，这样的策略称为**双向搜索**。
* 子集评价（subset evaluation）:

将特征子集搜索与子集评价机制结合，即可得到特征选择方法。

常见的特征选择方法大致可分为三类：过滤式（filter）、包裹式（wrapper）、和嵌入式（embedding）.

#### 过滤式选择

#### 包裹式选择

#### 嵌入式选择

### 稀疏表示与字典学习

### 压缩感知