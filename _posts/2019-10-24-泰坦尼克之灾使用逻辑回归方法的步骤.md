## 泰坦尼克之灾使用逻辑回归方法的步骤

pandas一般是存储格式化的数据以及对数据进行一些预处理等（数据分析）；

matplotlib、seaborn是对数据进行可视化，生成可视化的图表；

numpy是






### 1. 数据可视化（对数据的认识）

首先导入数据并熟悉数据的结构，分析数据由哪些特征构成以及特征的内在含义等

```python
data_train = pd.read_csv("/Users/Hanxiaoyang/Titanic_data/Train.csv")#导入pandas类型的数据
data_train.info()#描述数据的信息
data_train.describe()#描述数据的平均值、最大最小值等
```

然后使用matplotlib对数据进行绘图（关键在于绘图库的使用）

```python
fig = plt.figure()
fig.set(alpha=0.2)  # 设定图表颜色alpha参数
plt.subplot2grid((2,3),(0,0)) # 在一张大图里分列几个小图 第一个括号是几行几列，第二个括号是第几个
data_train.Survived.value_counts().plot(kind='bar')# 柱状图 采用直接对数据进行绘图的方法
plt.title(u"获救情况 (1为获救)") # 标题
plt.ylabel(u"人数") 

plt.subplot2grid((2,3),(1,0), colspan=2)
data_train.Age[data_train.Pclass == 1].plot(kind='kde')   
data_train.Age[data_train.Pclass == 2].plot(kind='kde')
data_train.Age[data_train.Pclass == 3].plot(kind='kde')
plt.xlabel(u"年龄")# plots an axis lable
plt.ylabel(u"密度") 
plt.title(u"各等级的乘客年龄分布")
plt.legend((u'头等舱', u'2等舱',u'3等舱'),loc='best') # sets our legend for our graph.

Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()
Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()
df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})
df.plot(kind='bar', stacked=True)
plt.title(u"各乘客等级的获救情况")
plt.xlabel(u"乘客等级") 
plt.ylabel(u"人数") 
plt.show()

```



### 2. 数据简单预处理

EDA(e)

一些数据处理的方法：

- removing Target column (id)（输入输出分离）
- Sampling (without replacement)（采样）
- Dealing with Imbalanced Data（处理不均衡数据）
- Introducing missing values and treating them (replacing by average values)（替换缺失数据）
- Noise filtering（噪声过滤）
- Data discretization（数据离散化）
- Normalization and standardization（标准化和归一化）
- PCA analysis（主成分分析）
- Feature selection (filter, embedded, wrapper)

#### 2.1 对缺失数据的处理

通常遇到缺值的情况，我们会有几种常见的处理方式：

* 如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了
* 如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中
* 如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。
* 有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上

> 这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据(注：RandomForest是一个用在原始数据中做不同采样，建立多颗DecisionTree，再进行average等等来降低过拟合现象，提高结果的机器学习算法

#### 2.2 特征因子化（Feature Encoding）

有一些特征不是数字类型的，或者是数字但没有数字含义的（如一、二、三等仓），需要将其转化为数字类型的，举个例子：

以Cabin为例，原本一个属性维度，因为其取值可以是[‘yes’,‘no’]，而将其平展开为’Cabin_yes’,'Cabin_no’两个属性

* 原本Cabin取值为yes的，在此处的"Cabin_yes"下取值为1，在"Cabin_no"下取值为0
* 原本Cabin取值为no的，在此处的"Cabin_yes"下取值为0，在"Cabin_no"下取值为1

我们使用pandas的"get_dummies"来完成这个工作，并拼接在原来的"data_train"之上

#### 2.3 数据归一化

当特征值的数据变化太大的时候，会对收敛速度产生很大影响，因此要对其进行归一化

用scikit-learn里面的preprocessing模块做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。

#### 2.4 特征清理



### 3.特征工程

**Features sorts:**

- numeric（数值）
- categorical（类别）
- ordinal(序列)
- datetime（时间）
- coordinates（坐标）

**Explorer Dataset:**

* Dimensions of the dataset（数据维度）.

* Peek at the data itself（窥探数据本身）.

* Statistical summary of all attributes（所有特性的统计总结）.

* Breakdown of the data by the class variable（按类变量细分数据）.

### 4. 建模

这里建立逻辑回归模型，首先需要把需要的feature字段提取出来，转换成numpy格式，使用scikit-learn中的LogisticRegression建模。

```python
from sklearn import linear_model
# 用正则取出我们要的属性值
train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')
train_np = train_df.as_matrix()

y = train_np[:, 0]# y即Survival结果
X = train_np[:, 1:]# X即特征属性值

# fit到RandomForestRegressor之中
clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)
clf.fit(X, y)
```

<font color=red>**在检验模型的结果时，需要用同样的方法对测试数据进行处理**</font>

一些机器学习算法：

- Classification
  - k-Nearest Neighbors（KNN，K最近邻）
  - LinearRegression
  - SVM（支持向量机）
  - DT
  - NN（神经网络）
- clustering
  - K-means（K均值）
  - HCA
  - Expectation Maximization
- Visualization and dimensionality reduction:
  - Principal Component Analysis(PCA)
  - Kernel PCA
  - Locally -Linear Embedding (LLE)
  - t-distributed Stochastic NeighborEmbedding (t-SNE)
- Association rule learning
  - Apriori
  - Eclat
- Semisupervised learning
- Reinforcement Learning
  - Q-learning
- Batch learning & Online learning
- Ensemble Learning

### 5. 模型优化

#### 4.1 模型系数关联分析

#### 4.2 交叉验证

把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。

我们用scikit-learn的cross_validation来帮我们完成小数据集上的这个工作。

#### 4.3 学习曲线

解决过拟合和欠拟合的问题

### 6. 模型融合（model ensemble）

最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上**有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)**，那我们让他们**都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果**。







### 一些经验

1. 应用机器学习，千万不要一上来就试图做到完美，先撸一个baseline的model出来，再进行后续的分析步骤，一步步提高，所谓后续步骤可能包括『分析model现在的状态(欠/过拟合)，分析我们使用的feature的作用大小，进行feature selection，以及我们模型下的bad case和产生的原因』等等。

2. Kaggle上的大神们，也分享过一些experience：

   『对**数据的认识**太重要了！』
   『数据中的**特殊点/离群点的分析和处理**太重要了！』
   『**特征工程(feature engineering)**太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』
   『要做**模型融合(model ensemble)**』

### 参考

[链接（中文）](https://blog.csdn.net/han_xiaoyang/article/details/49797143)

[A Comprehensive ML Workflow with Python](https://www.kaggle.com/kernels/scriptcontent/17428415/download)