# 强化学习（增强学习）

### 知识联系

强化学习是属于机器学习的一种，机器学习主要分监督学习、非监督学习、半监督学习、增强学习。

### 定义及意义、目的

描述：

> 强化学习任务通常用**马尔可夫决策过程(Markov Decision Process, 简称MDP)**来描述:机器处于环境E中，**状态空间为X**,其中每个状态x∈x是机器感知到的环境的描述，如在种瓜任务上这就是当前瓜苗长势的描述;机器能采取的动作构成了**动作空间A**,如种瓜过程中有浇水、施不同的肥、使用不同的农药等多种可供选择的动作;若某个动作a∈A作用在当前状态x.上,则潜在的**转移函数P**将使得环境从当前状态按某种概率转移到另一个状态,如瓜苗状态为缺水，若选择动作浇水，则瓜苗长势会发生变化，瓜苗有一定的概率恢复健康,也有- -定的概率无法恢复;在转移到另--个状态的同时，环境会根据潜在的**“奖赏”(reward)函数R**反馈给机器- -个奖赏,如保持瓜苗健康对应奖赏+1,瓜苗凋零对应奖赏- -10,最终种出了好瓜对应奖赏+100.综合起来，强化学习任务对应了四元组E= (X,A,P, R>,其中P:Xx AxXr + R指定了状态转移概率, R:X xAxXHR指定了奖赏;在有的应用中，奖赏函数可能仅与状态转移有关，即R:Xx XHR.
>
> **状态空间** **动作空间**导致状态空间依**转移函数**改变  状态空间改变会产生**奖赏**
>
> 机器要做的是通过在环境中不断尝试而学得一个策略，根据这个策略在状态x下就知道要执行的动作a。在强化学习任务中，学习的目的就是要找到使长期积累奖赏最大化的策略
>
> 策略的优劣取决于长期执行这一策略后得到的累计奖赏
>
> 
>
> 强化学习与监督学习的对比中：状态对应实例，动作对应标记   策略相当于分类器（离散）或回归器（连续），因此，在某种意义上，强化学习可以看做具有“延迟标记信息”的样本
>
> from:周志华《机器学习》

> 我们可以用增强学习让计算机自己去学着做事情，比如说让计算机学着去玩Flappy Bird，或者让计算机去计算数据，我们需要做的，我们不需要设置具体的策略，比如先飞到上面，再飞到下面，我们只是需要给算法定一个“小目标”！比如当计算机玩的好的时候，我们就给它一定的奖励，它玩的不好的时候，就给它一定的惩罚，在这个算法框架下，它就可以越来越好，超过人类玩家的水平。
> 原文链接：https://blog.csdn.net/superCally/article/details/54754787

学习目的：针对一些问题更好的进行决策

> **1.强化学习是什么？与其它机器学习方法有什么关系？**
>
> 强化学习是一种机器学习方法，它使Agent能够在交互式环境中年通过试验并根据自己的行动和经验反馈的错误来进行学习。虽然监督学习和强化学习都使用输入和输出之间的映射关系，但强化学习与监督学习不同，监督学习提供给Agent的反馈是执行任务的正确行为，而强化学习使用奖励和惩罚作为积极和消极行为的信号。
>
> 与无监督学习相比而言，强化学习在目标方面有所不同。虽然无监督学习的目标是找出数据点之间的相似性和不同性，但是在强化学习中，其目标是找到一个合适的动作模型，能够最大化Agent的累积奖励总额。下图表示了强化学习模型中涉及的基本思想和要素。
>
> 原文链接：https://www.jianshu.com/p/9526f944ca6f

![](https://upload-images.jianshu.io/upload_images/2509688-0751a2fd5eb39410.png?imageMogr2/auto-orient/strip|imageView2/2/w/700/format/webp)




### 强化学习原理



### 强化学习的分类

按给定条件，强化学习可分为**基于模式的强化学习（model-based RL）**和**无模式强化学习（model-free RL）** ，以及**主动强化学习（active RL）**和**被动强化学习（passive RL）** 。

强化学习的变体包括**逆向强化学习**、**阶层强化学习**和**部分可观测系统**的强化学习。

求解强化学习问题所使用的算法可分为**策略搜索算法**和**值函数（value function）算法**两类。

深度学习模型可以在强化学习中得到使用，形成深度强化学习 。



Q-learning和SARSA（State-Action-Reward-State-Action）是两种常用的**无模式（model-free）强化学习**算法。

### 举例分析

### 强化学习有哪些实际应用？

适用于决策性的问题

由于增强学习需要大量的数据，因此它最适用于模拟数据领域，比如游戏、机器人等。

在电脑游戏中，增强学习被广泛地应用于人工智能的构建中。AlphaGo Zero是围棋界第一个击败世界冠军的计算机程序，类似的还有ATARI游戏、西洋双陆棋等。

在机器人和工业自动化领域，增强学习被用于使机器人为其自身创建一个高效的自适应控制系统，从而能够**从自己的经验和行为中学习**。DeepMind在深度增强学习上的成果也是一个很好的例子。

增强学习的其它应用包括文本摘要引擎、对话代理（文本、语言），它们可以从用户交互中学习，并随着时间的推移而不断改进。此外，对于医疗保健和在线股票交易而言，基于增强学习的性能也是最佳的。















强化学习中有两种重要的方法：**Policy Gradients和Q-learning**。其中Policy Gradients方法直接预测在某个环境下应该采取的Action，而Q-learning方法预测某个环境下所有Action的期望值(即Q值）。一般来说，Q-learning方法只适合有少量离散取值的Action环境，而Policy Gradients方法适合有连续取值的Action环境。在与深度学习方法结合后，这两种算法就变成了Policy Network和DQN(Deep Q-learning Network)。



## Gym

Gym 是 OpenAI 发布的用于开发和比较强化学习算法的工具包。使用它我们可以让 AI 智能体做很多事情，比如行走、跑动，以及进行多种游戏。**提供强化学习所需要的环境**









