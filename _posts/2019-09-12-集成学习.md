## 集成学习

[TOC]

### 定义

集成学习通过构建并结合多个学习器来完成学习任务，有时也称为多分类器系统。

如何产生“好而不同”的个体学习器是集成学习的核心。

### 分类

集成学习分为**同质**和**异质**，同质指的是只包含同种类型的学习器（比如决策树，BP神经网络等），此时个体学习器称为`基学习器`，相应的学习算法称为`基学习算法`；异质指的是包含多种学习器，个体学习器常称为`组件学习器`，或直接称为`个体学习算法`，而没有`基学习算法`这个概念。

根据个体学习器的生成方式，目前的集成学习方法大概分为两大类：

* 个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting
* 个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（Random Forest）

#### 1. Boosting

这是一族可将弱学习器提升为强学习器的算法。其工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复运行直至基学习器数目达到事先指定的值T，最终对这T个基学习器进行加权平均。

Boosting族最著名的代表是AdaBoost

#### 2. Bagging与随机森林

欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立;虽然“独立”在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集,一种可能的做法是对训练样本进行采样,产生出若干个不同的子集,再从每个数据子集中训练出一个基学习器.这样,由于训练数据不同,我们获得的基学习器可望具有比较大的差异.然而,为获得好的集成，我们同时还希望个体学习器不能太差.如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据,甚至不足以进行有效学习,这显然无法确保产生出比较好的基学习器.为解决这个问题,我们可考虑使用相互有交叠的采样子集.

##### 2.1 Bagging

Bagging [Breiman, 1996a]是并行式集成学习方法最著名的代表.从名字即可看出,它直接基于自助采样法(bootstrap sampling)。给定包含m个样本的数据集,我们先随机取出一一个样本放入采样集中，再把该样本放回初始数据集,使得下次采样时该样本仍有可能被选中，这样，经过m 次随机采样操作,我们得到含m个样本的采样集,初始训练集中有的样本在采样集里多次出现，有的则从未出现.由式(2.1)可知,初始训练集中约有63.2%的样本出现在采样集中.
照这样,我们可采样出T个含m个训练样本的采样集,然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging的基本流程.在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法,对回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学 习器投票的置信度来确定最终胜者. 

##### 2.2 随机森林

随机森林（RF）是Bagging的一个扩展变体。RF在以决策树为基学习构建Bagging的基础上，进一步在决策树的训练过程中引入随机属性选择。

### 目的